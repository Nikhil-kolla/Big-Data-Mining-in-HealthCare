# -*- coding: utf-8 -*-
"""Diabetes_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17hPhlsj9pN9plaZsWewdaisVCHZeHjIm
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings("ignore")
from imblearn.over_sampling import ADASYN
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

"""Import PIMA Indian Diabetes Dataset"""

data = pd.read_csv("/content/drive/My Drive/BDMH/diabetes.csv")
data.head()

data.columns

data.info()

x = data[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age']]
y = data['Outcome']

"""Imutation"""

def imputation(data,feature):
  median = data[feature].median()
  print("median of "+str(feature)+"::"+str(median))
  data = data.replace({feature:0},median)
  return data

data = imputation(data,"BloodPressure")
data = imputation(data,"Glucose")
data = imputation(data,"SkinThickness")
data = imputation(data,"Insulin")
## we can still see zeroes in insulin even after imputation for diabetic samples..so we can roughly conclude that diabetics have very low insulin levels..
data = imputation(data,"BMI")

"""OverSampling of data"""

## oversampling is performed only on training data...
oversample = ADASYN(random_state=10,sampling_strategy="minority")
x_train_os, y_train_os = oversample.fit_sample(x_train,y_train)
dict_={"Non-Diabetic":0,"Diabetic":0}
for i in y_train_os:
  if(i==0):
    dict_['Non-Diabetic']+=1
  else:
    dict_['Diabetic']+=1
plt.bar(*zip(*dict_.items()))
plt.title("COUNT OF SAMPLES IN TRAINING DATA PER CLASS(AFTER OVERSAMPLING)")
plt.ylabel("Count of Samples")
plt.show()

"""Split Data"""

X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=0)

print("Shape of X train data : ",X_train.shape)
print("Shape of X test data : ",X_test.shape)
print("Shape of Y train data : ",Y_train.shape)
print("Shape of Y test data : ",Y_test.shape)

"""Normalization"""

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""### SVM Linear"""

from sklearn import svm
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
import pickle 
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score

svm_model = Pipeline([('clf', SVC())])
param_C = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
param_scoring = ["accuracy","precision","recall","f1_macro"]

param = [{'clf__C': param_C,'clf__kernel': ['linear']}]
grid = GridSearchCV(estimator=svm_model,param_grid=param,refit = 'accuracy',scoring=param_scoring,cv=10,n_jobs=-1)
grid = grid.fit(X_train,Y_train)

# grid.cv_results_

cv_prec_svmlinear = grid.cv_results_['mean_test_precision']
cv_recall_svmlinear = grid.cv_results_['mean_test_recall']
cv_fscore_svmlinear = grid.cv_results_['mean_test_f1_macro']
cv_accs_svmlinear = grid.cv_results_['mean_test_accuracy']

"""Single Plot"""

plt.plot(cv_prec_svmlinear,label="Precision on CV data")
plt.plot(cv_recall_svmlinear,label= "Recall on CV data")
plt.plot(cv_fscore_svmlinear,label = "F-Score on CV data")
plt.plot(cv_accs_svmlinear,label = "Accuracy on CV data")

cval = [0.0001, 0.001 , 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
y_pos = np.arange(len(cval))
plt.xticks(y_pos,cval)

plt.xlabel("Value of C")
plt.ylabel("PERFORMANCE ON CV DATA")
plt.title("Value of C  VS PERFORMANCE ON CV DATA : SVM Linear")
plt.legend()
plt.grid(linestyle='-')
plt.show()

"""Plotting Graphs based on different Values of C vs Different score On CV Data

Values of C vs Precision
"""

plt.plot(cv_prec_svmlinear)

cval = [0.0001, 0.001 , 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
y_pos = np.arange(len(cval))
plt.xticks(y_pos,cval)

plt.xlabel("Value of C")
plt.ylabel("PRECISION ON CV DATA")
plt.title("Value of C  V/S CV PRECISION : SVM Linear")
plt.grid(linestyle="-")
plt.show()

"""Values of C vs Recall"""

plt.plot(cv_recall_svmlinear)

cval = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
y_pos = np.arange(len(cval))
plt.xticks(y_pos,cval)

plt.xlabel("Value of C")
plt.ylabel("RECALL ON CV DATA")
plt.title("Value of C  V/S CV RECALL : SVM Linear")
plt.grid(linestyle="-")
plt.show()

"""Value of C vs FScore"""

plt.plot(cv_fscore_svmlinear)

cval = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
y_pos = np.arange(len(cval))
plt.xticks(y_pos,cval)

plt.xlabel("Value of C")
plt.ylabel("FScore ON CV DATA")
plt.title("Value of C  V/S CV FScore : SVM Linear")
plt.grid(linestyle="-")
plt.show()

"""Value of C vs Accuracy"""

plt.plot(cv_accs_svmlinear)

cval = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
y_pos = np.arange(len(cval))
plt.xticks(y_pos,cval)

plt.xlabel("Value of C")
plt.ylabel("Accuracy ON CV DATA")
plt.title("Value of C  V/S CV Accuracy : SVM Linear")
plt.grid(linestyle="-")
plt.show()

"""Optimal SVM Linear Model"""

print(" The Accuracy of the best model is : ",grid.best_score_)
print(" Best Parameters : ",grid.best_params_)

classifier = grid.best_estimator_
classifier.fit(X_train, Y_train)
Y_pred = classifier.predict(X_test)

classifier_svmlinear = classifier

print("Accuracy on test data using SVM Linear::",classifier.score(X_test, Y_test)*100)
print("Precision on test data using SVM Linear::",precision_score(Y_test, Y_pred, average="macro"))
print("Recall on test data using SVM Linear::",recall_score(Y_test, Y_pred, average="macro"))
print("F-Score on test data using SVM Linear::",f1_score(Y_test, Y_pred, average="macro"))
spec = confusion_matrix(Y_test, Y_pred)[1][1]/(confusion_matrix(Y_test, Y_pred)[0][1]+confusion_matrix(Y_test, Y_pred)[1][1])
print("Specificity on test data using SVM Linear::",spec)

"""Saving the Optimal Model SVMLinear"""

# pickle_out = open("/content/drive/My Drive/BDMH/svmlinear_78_acc.pickle","wb")
# pickle.dump(classifier_svmlinear, pickle_out)

"""### SVM RBF"""

svm_model = Pipeline([('clf', SVC())])
param_C = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
param_scoring = ["accuracy","precision","recall","f1_macro"]

param = [{'clf__C': param_C,'clf__kernel': ['rbf']}]
grid_rbf = GridSearchCV(estimator=svm_model,param_grid=param,refit = 'accuracy',scoring=param_scoring,cv=10,n_jobs=-1)
grid_rbf = grid_rbf.fit(X_train,Y_train)

cv_prec_svmrbf = grid_rbf.cv_results_['mean_test_precision']
cv_recall_svmrbf= grid_rbf.cv_results_['mean_test_recall']
cv_fscore_svmrbf = grid_rbf.cv_results_['mean_test_f1_macro']
cv_accs_svmrbf = grid_rbf.cv_results_['mean_test_accuracy']

"""Single Plot"""

plt.plot(cv_prec_svmrbf,label="Precision on CV data")
plt.plot(cv_recall_svmrbf,label= "Recall on CV data")
plt.plot(cv_fscore_svmrbf,label = "F-Score on CV data")
plt.plot(cv_accs_svmrbf,label = "Accuracy on CV data")

cval = [0.0001, 0.001 , 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
y_pos = np.arange(len(cval))
plt.xticks(y_pos,cval)

plt.xlabel("Value of C")
plt.ylabel("PERFORMANCE ON CV DATA")
plt.title("Value of C  VS PERFORMANCE ON CV DATA : SVM RBF")
plt.legend()
plt.grid(linestyle='-')
plt.show()

"""Plotting Graphs based on different Values of C vs Different score On CV Data

Values of C vs Precision
"""

plt.plot(cv_prec_svmrbf)

cval = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
y_pos = np.arange(len(cval))
plt.xticks(y_pos,cval)

plt.xlabel("Value of C")
plt.ylabel("PRECISION ON CV DATA")
plt.title("Value of C  V/S CV PRECISION : SVM RBF")
plt.grid(linestyle="-")
plt.show()

"""Values of C vs Recall"""

plt.plot(cv_recall_svmrbf)

cval = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
y_pos = np.arange(len(cval))
plt.xticks(y_pos,cval)

plt.xlabel("Value of C")
plt.ylabel("Recall ON CV DATA")
plt.title("Value of C  V/S CV RECALL : SVM RBF")
plt.grid(linestyle="-")
plt.show()

"""Values of C vs FScore"""

plt.plot(cv_fscore_svmrbf)

cval = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
y_pos = np.arange(len(cval))
plt.xticks(y_pos,cval)

plt.xlabel("Value of C")
plt.ylabel("Fscore ON CV DATA")
plt.title("Value of C  V/S CV Fscore : SVM RBF")
plt.grid(linestyle="-")
plt.show()

"""Values of C vs Accuracy"""

plt.plot(cv_accs_svmrbf)

cval = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
y_pos = np.arange(len(cval))
plt.xticks(y_pos,cval)

plt.xlabel("Value of C")
plt.ylabel("Accuracy ON CV DATA")
plt.title("Value of C  V/S CV Accuracy : SVM RBF")
plt.grid(linestyle="-")
plt.show()

"""Optimal SVM RBF Model"""

print(" The Accuracy of the best model is : ",grid_rbf.best_score_)
print(" Best Parameters : ",grid_rbf.best_params_)

classifier_rbf = grid_rbf.best_estimator_
classifier_rbf.fit(X_train, Y_train)
Y_pred = classifier_rbf.predict(X_test)

print("Accuracy on test data using SVM RBF::",classifier_rbf.score(X_test, Y_test)*100)
print("Precision on test data using SVM RBF::",precision_score(Y_test, Y_pred, average="macro"))
print("Recall on test data using SVM RBF::",recall_score(Y_test, Y_pred, average="micro"))
print("F-Score on test data using SVM RBF::",f1_score(Y_test, Y_pred, average="micro"))
spec = confusion_matrix(Y_test, Y_pred)[1][1]/(confusion_matrix(Y_test, Y_pred)[0][1]+confusion_matrix(Y_test, Y_pred)[1][1])
print("Specificity on test data using SVM RBF::",spec)

"""Saving the optimal Model"""

# pickle_out = open("/content/drive/My Drive/BDMH/svmrbf_75_acc.pickle","wb")
# pickle.dump(classifier_rbf, pickle_out)



"""### Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

param = {"criterion":["gini"] , "max_depth":[1,2,3,4,5,6,7,8,9,10] }
clf_dt = DecisionTreeClassifier(random_state=45)
param_scoring = ["accuracy","precision","recall","f1_macro"]

grid_dt = GridSearchCV(clf_dt,param,scoring=param_scoring,refit='accuracy',cv=10,n_jobs=-1)
grid_dt.fit(X_train,Y_train)

cv_prec_dt = grid_dt.cv_results_['mean_test_precision']
cv_recall_dt = grid_dt.cv_results_['mean_test_recall']
cv_fscore_dt = grid_dt.cv_results_['mean_test_f1_macro']
cv_accs_dt = grid_dt.cv_results_['mean_test_accuracy']

"""Single Plot"""

plt.plot(cv_prec_dt,label="Precision on CV data")
plt.plot(cv_recall_dt,label= "Recall on CV data")
plt.plot(cv_fscore_dt,label = "F-Score on CV data")
plt.plot(cv_accs_dt,label = "Accuracy on CV data")

cval = [1,2,3,4,5,6,7,8,9,10]
y_pos = np.arange(len(cval))
plt.xticks(y_pos,cval)

plt.xlabel("Max Depth")
plt.ylabel("PERFORMANCE ON CV DATA")
plt.title("Max Depth VS PERFORMANCE ON CV DATA : Decision Tree")
plt.legend()
plt.grid(linestyle='-')
plt.show()

"""Plotting Graphs based on Max Depth vs score On CV Data

Max Depth vs Precision
"""

plt.plot(cv_prec_dt)

cval = [1,2,3,4,5,6,7,8,9,10]
y_pos = np.arange(len(cval))
plt.xticks(y_pos,cval)

plt.xlabel("Max depth")
plt.ylabel("Precision ON CV DATA")
plt.title("Depth  V/S CV Precision : Decision Tree")
plt.grid(linestyle="-")
plt.show()

"""Max Depth vs Recall"""

plt.plot(cv_recall_dt)

cval = [1,2,3,4,5,6,7,8,9,10]
y_pos = np.arange(len(cval))
plt.xticks(y_pos,cval)

plt.xlabel("Max depth")
plt.ylabel("Recall ON CV DATA")
plt.title("Depth  V/S CV Recall : Decision Tree")
plt.grid(linestyle="-")
plt.show()

"""Max Depth vs F-Score"""

plt.plot(cv_fscore_dt)

cval = [1,2,3,4,5,6,7,8,9,10]
y_pos = np.arange(len(cval))
plt.xticks(y_pos,cval)

plt.xlabel("Max depth")
plt.ylabel("F-Score ON CV DATA")
plt.title("Depth  V/S CV F-score: Decision Tree")
plt.grid(linestyle="-")
plt.show()

max(cv_fscore_dt)

"""Max Depth vs Accuracy"""

plt.plot(cv_accs_dt)

cval = [1,2,3,4,5,6,7,8,9,10]
y_pos = np.arange(len(cval))
plt.xticks(y_pos,cval)

plt.xlabel("Max depth")
plt.ylabel("Accuracy ON CV DATA")
plt.title("Depth  V/S CV Accuracy: Decision Tree")
plt.grid(linestyle="-")
plt.show()

"""Optimal Decision Tree"""

print(" The Accuracy of the best model is : ",grid_dt.best_score_)
print(" Best Parameters : ",grid_dt.best_params_)

classifier_dt = grid_dt.best_estimator_
classifier_dt.fit(X_train, Y_train)
Y_pred = classifier_dt.predict(X_test)

print("Accuracy on test data using Decision Tree::",classifier_dt.score(X_test, Y_test)*100)
print("Precision on test data using Decision Tree::",precision_score(Y_test, Y_pred, average="macro"))
print("Recall on test data using Decision Tree::",recall_score(Y_test, Y_pred, average="micro"))
print("F-Score on test data using Decision Tree::",f1_score(Y_test, Y_pred, average="micro"))
spec = confusion_matrix(Y_test, Y_pred)[1][1]/(confusion_matrix(Y_test, Y_pred)[0][1]+confusion_matrix(Y_test, Y_pred)[1][1])
print("Specificity on test data using Decision Tree::",spec)



"""Saving the optimal Model"""

# pickle_out = open("/content/drive/My Drive/BDMH/dt_72_acc.pickle","wb")
# pickle.dump(classifier_dt, pickle_out)

