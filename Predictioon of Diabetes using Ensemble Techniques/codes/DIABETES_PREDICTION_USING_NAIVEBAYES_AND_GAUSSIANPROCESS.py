# -*- coding: utf-8 -*-
"""bdmh_proj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KpelBYvPpIbSNZYOOYq8t0hfcWXjDUzq

### Loading Libraries
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import warnings
warnings.filterwarnings("ignore")
from imblearn.over_sampling import ADASYN
from tqdm import tqdm
import pickle 
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score

from sklearn.model_selection import GridSearchCV
from sklearn.naive_bayes import GaussianNB

"""### Reading data"""

x_train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/bdmh/xtrain.csv')
y_train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/bdmh/ytrain.csv')
x_test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/bdmh/xtest.csv')
y_test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/bdmh/ytest.csv')

print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

xcols = x_train.columns
x_train = x_train.drop(xcols[0], axis = 1)
x_test = x_test.drop(xcols[0], axis = 1)

cols = y_train.columns
y_train = y_train.drop(cols[0], axis = 1)
y_test = y_test.drop(cols[0], axis = 1)

print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

"""### Over Sampling Of Training Data"""



## oversampling is performed only on training data...
oversample = ADASYN(random_state=10,sampling_strategy="minority")
x_train_os, y_train_os = oversample.fit_sample(x_train,y_train)
dict_={"Non-Diabetic":0,"Diabetic":0}
for i in y_train_os:
  if(i==0):
    dict_['Non-Diabetic']+=1
  else:
    dict_['Diabetic']+=1
plt.bar(*zip(*dict_.items()))
plt.title("COUNT OF SAMPLES IN TRAINING DATA PER CLASS(AFTER OVERSAMPLING)")
plt.ylabel("Count of Samples")
plt.show()

scale = StandardScaler()
x_train_std = scale.fit_transform(x_train_os)
x_test_std = scale.transform(x_test)

"""### Guassian Naive Bayes (with over sampling Minority Class)

### Fine tuning Of hyper parameter using Grid Search CV
"""

scoring  = ["accuracy","precision","recall","f1_macro"]
var = [1e-9,1e-8,1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1,1e-0]

parameters = {'var_smoothing':var}
gnb_os = GaussianNB()
clf_gnb_os = GridSearchCV(gnb_os,parameters,cv=5,refit="accuracy",scoring=scoring)
clf_gnb_os.fit(x_train_std, y_train_os)

clf_gnb_os.best_estimator_

cv_prec_gnb_os = clf_gnb_os.cv_results_['mean_test_precision']
cv_recall_gnb_os = clf_gnb_os.cv_results_['mean_test_recall']
cv_fscore_gnb_os = clf_gnb_os.cv_results_['mean_test_f1_macro']
cv_accs_gnb_os = clf_gnb_os.cv_results_['mean_test_accuracy']

prec_index_gnb_os = np.where(cv_prec_gnb_os == max(cv_prec_gnb_os))[0][0]
recall_index_gnb_os = np.where(cv_recall_gnb_os == max(cv_recall_gnb_os))[0][0] 
f_score_index_gnb_os = np.where(cv_fscore_gnb_os == max(cv_fscore_gnb_os))[0][0] 
acc_index_gnb_os = np.where(cv_accs_gnb_os == max(cv_accs_gnb_os))[0][0]


print(f'Highest Precision is : {max(cv_prec_gnb_os)}, when Variance is : {var[prec_index_gnb_os]}')
print(f'Highest Recall is : {max(cv_recall_gnb_os)}, when Variance is : {var[recall_index_gnb_os]}')
print(f'Highest F Score is : {max(cv_fscore_gnb_os)}, when Variance is : {var[f_score_index_gnb_os]}')
print(f'Highest Accuracy is : {max(cv_accs_gnb_os)}, when Variance is : {var[acc_index_gnb_os]}')

"""### Guassian Naive Bayes without Over sampling(Original data)"""

scale = StandardScaler()
x_train = scale.fit_transform(x_train)
x_test = scale.transform(x_test)

scoring  = ["accuracy","precision","recall","f1_macro"]
var = [1e-9,1e-8,1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1,1e-0]

parameters = {'var_smoothing':var}
gnb = GaussianNB()
clf_gnb = GridSearchCV(gnb_os,parameters,cv=5,refit="accuracy",scoring=scoring)
clf_gnb.fit(x_train, y_train)

cv_prec_gnb = clf_gnb.cv_results_['mean_test_precision']
cv_recall_gnb = clf_gnb.cv_results_['mean_test_recall']
cv_fscore_gnb = clf_gnb.cv_results_['mean_test_f1_macro']
cv_accs_gnb = clf_gnb.cv_results_['mean_test_accuracy']

prec_index_gnb = np.where(cv_prec_gnb == max(cv_prec_gnb))[0][0]
recall_index_gnb = np.where(cv_recall_gnb == max(cv_recall_gnb))[0][0] 
f_score_index_gnb = np.where(cv_fscore_gnb == max(cv_fscore_gnb))[0][0] 
acc_index_gnb = np.where(cv_accs_gnb == max(cv_accs_gnb))[0][0]


print(f'Highest Precision is : {max(cv_prec_gnb)}, when Variance is : {var[prec_index_gnb]}')
print(f'Highest Recall is : {max(cv_recall_gnb)}, when Variance is : {var[recall_index_gnb]}')
print(f'Highest F Score is : {max(cv_fscore_gnb)}, when Variance is : {var[f_score_index_gnb]}')
print(f'Highest Accuracy is : {max(cv_accs_gnb)}, when Variance is : {var[acc_index_gnb]}')

clf_gnb.best_estimator_

"""### PLOTS"""

plt.title(label= 'Variance vs Metrics')
plt.plot(cv_prec_gnb, label = 'Precision')
plt.plot(cv_recall_gnb, label = 'Recall')
plt.plot(cv_fscore_gnb, label = 'F Score')
plt.plot(cv_accs_gnb, label = 'Accuracy')
plt.legend()
plt.xlabel = 'Variance'
plt.ylabel = 'Performance on CV data'
plt.xticks(np.arange(len(var)), var)
plt.grid()
plt.show()

"""### Best GNB Classifier (without Over Sampling)"""

final_GNB_wos = GaussianNB(var_smoothing = 0.1)
final_GNB_wos.fit(x_train, y_train)


y_pred = final_GNB_wos.predict(x_test)
acc = final_GNB_wos.score(x_test, y_test)*100
prec = precision_score(y_test, y_pred, average="macro")
recall = recall_score(y_test, y_pred, average="macro")
f_score =f1_score(y_test, y_pred, average="macro")
spec = confusion_matrix(y_test, y_pred)[1][1]/(confusion_matrix(y_test, y_pred)[0][1]+confusion_matrix(y_test, y_pred)[1][1])

print('Accuracy : ', acc)
print('Precision : ', prec)
print('Recall : ', recall)
print('F Score : ', f_score)
print('Specificity : ', spec)

with open('/content/drive/My Drive/Colab Notebooks/bdmh/best_gnb_clf.pkl', 'wb') as w:
    pickle.dump(final_GNB_wos, w)

"""### Guassian Process Classifier (without over sampling)"""

from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF

kernel = 1.0 * RBF(1.0)
gpc = GaussianProcessClassifier(kernel=kernel, random_state=0).fit(x_train, y_train)


y_pred = gpc.predict(x_test)
acc = gpc.score(x_test, y_test)*100
prec = precision_score(y_test, y_pred, average="macro")
recall = recall_score(y_test, y_pred, average="macro")
f_score=f1_score(y_test, y_pred, average="macro")
spec = confusion_matrix(y_test, y_pred)[1][1]/(confusion_matrix(y_test, y_pred)[0][1]+confusion_matrix(y_test, y_pred)[1][1])

print('Accuracy : ', acc)
print('Precision : ', prec)
print('Recall : ', recall)
print('F Score : ', f_score)
print('Specificity : ', spec)

"""### GPC with over sampling"""



from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF

kernel = 1.0 * RBF(1.0)
gpc_os = GaussianProcessClassifier(kernel=kernel, random_state=0).fit(x_train_std, y_train_os)


y_pred_os = gpc_os.predict(x_test_std)
acc_os = gpc_os.score(x_test_std, y_test)*100
prec_os = precision_score(y_test, y_pred_os, average="macro")
recall_os = recall_score(y_test, y_pred_os, average="macro")
f_score_os =f1_score(y_test, y_pred_os, average="macro")
spec_os = confusion_matrix(y_test, y_pred_os)[1][1]/(confusion_matrix(y_test, y_pred_os)[0][1]+confusion_matrix(y_test, y_pred_os)[1][1])

print('Accuracy : ', acc_os)
print('Precision : ', prec_os)
print('Recall : ', recall_os)
print('F Score : ', f_score_os)
print('Specificity : ', spec_os)

metrics = [acc,prec*100, recall*100, f_score*100, spec*100]
objects = ('Acc', 'prec', 'Recall', 'f score', 'spec',)
y_pos = np.arange(len(objects))


plt.bar(y_pos, metrics, align='center', alpha=0.5)
plt.xticks(y_pos, objects)
plt.ylabel = 'Percentage'
plt.title('Different Metrics of Guassian Process Classifier')

plt.show()

"""### Best GPC Classifier"""

with open('/content/drive/My Drive/Colab Notebooks/bdmh/best_gnb_clf.pkl', 'wb') as w:
    pickle.dump(gpc, w)

