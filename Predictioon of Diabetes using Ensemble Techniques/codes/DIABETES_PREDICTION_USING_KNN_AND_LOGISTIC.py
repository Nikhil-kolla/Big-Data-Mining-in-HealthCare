# -*- coding: utf-8 -*-
"""BDMH Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nxPff0jk6edWXMG_R-suaSjztUTjarUm

BDMH Project Winter 2020

Importing Required Libraries
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
import numpy as np
import time
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import warnings
warnings.filterwarnings("ignore")
from imblearn.over_sampling import ADASYN
from tqdm import tqdm
import pickle 
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline

"""Reading data"""

from google.colab import drive
drive.mount('/content/gdrive')

data = pd.read_csv('/content/gdrive/My Drive/BDMH/Project/diabetes.csv')
data.head()

"""Seperating out samples and labels"""

data_y = pd.DataFrame(data = data['Outcome'],columns=['Outcome'])
data_x = data.drop(columns="Outcome")

"""Feature Columns"""

data_x.columns

data_y.columns

"""Performing EDA

Count of samples per class in the dataset
"""

data_y['Outcome'].value_counts().plot(kind='bar')
plt.title("INDIVIDUAL COUNT OF LABELS IN THE DATA")
#plt.grid()
plt.xticks([0, 1],["Non-Diabetic","Diabetic"],rotation=360)
plt.ylabel("Count of Samples")
plt.show()

"""Histogram of given data"""

data_x.hist(figsize=(16,9), edgecolor="black", bins=20)
plt.show()

"""Performing Imputation on spurious features 'Glucose','BMI','Insulin','BloodPressur','Skin Thickness'"""

#ref:https://towardsdatascience.com/pima-indian-diabetes-prediction-7573698bd5fe
# def imputation(data,feature):
#   df1 = data.loc[data['Outcome'] == 1]
#   df2 = data.loc[data['Outcome'] == 0]
#   df1 = df1.replace({feature:0}, np.median(df1[feature]))
#   df2 = df2.replace({feature:0}, np.median(df2[feature]))
#   dataframe = [df1, df2]
#   dataset = pd.concat(dataframe)
#   return dataset

def imputation(data,feature):
  median = data[feature].median()
  print("median of "+str(feature)+"::"+str(median))
  data = data.replace({feature:0},median)
  return data

data = imputation(data,"BloodPressure")
data = imputation(data,"Glucose")
data = imputation(data,"SkinThickness")
data = imputation(data,"Insulin")
## we can still see zeroes in insulin even after imputation for diabetic samples..so we can roughly conclude that diabetics have very low insulin levels..
data = imputation(data,"BMI")

"""Univariate Analysis for every feature"""

def Univar_analysis(data,feature):
  data_nd = data.loc[data["Outcome"]==0] 
  data_d = data.loc[data["Outcome"]==1] 
  sb.set_style("whitegrid") 
  plt.xlabel(feature)
  plt.plot(data_nd[feature],np.zeros_like(data_nd[feature]),marker="x",label='Non-Diabetic',color='red') ## zeros_like will simply create zeros with length same as x cordinate.. 
  plt.plot(data_d[feature],np.zeros_like(data_d[feature]),marker="+",label='Diabetic',color='green') 
  plt.legend()
  plt.title("1-D Scatter plot on feature "+feature)
  plt.show()

Univar_analysis(data,"Age")

Univar_analysis(data,"Pregnancies")

Univar_analysis(data,"Glucose")

Univar_analysis(data,"BloodPressure")

Univar_analysis(data,"SkinThickness")

Univar_analysis(data,"Insulin")

Univar_analysis(data,"BMI")

Univar_analysis(data,"DiabetesPedigreeFunction")

"""INFERENCE
No single feature can seperate out or atleast nearly classify one class from the another, because there is considerable amount of overlap between the points of both the classes in all the features.

Pair Plot
"""

sb.pairplot(data,hue="Outcome")
plt.show()

"""Splitting the data into train and test"""

x_train,x_test,y_train,y_test = train_test_split(data_x,data_y,train_size=0.70,random_state=0)#45
print("SHAPE OF X-TRAIN::",x_train.shape)
print("SHAPE OF Y-TRAIN::",y_train.shape)
print("SHAPE OF X-TEST::",x_test.shape)
print("SHAPE OF Y-TEST::",y_test.shape)
y_train['Outcome'].value_counts().plot(kind='bar')
plt.title("COUNT OF SAMPLES IN TRAINING DATA PER CLASS(BEFORE OVERSAMPLING)")
plt.xticks([0, 1],["Non-Diabetic","Diabetic"],rotation=360)
plt.ylabel("Count of Samples")
plt.show()

y_train.head()

"""Oversampling the data"""

## oversampling is performed only on training data...
oversample = ADASYN(random_state=10,sampling_strategy="minority")
x_train_os, y_train_os = oversample.fit_sample(x_train,y_train)
dict_={"Non-Diabetic":0,"Diabetic":0}
for i in y_train_os:
  if(i==0):
    dict_['Non-Diabetic']+=1
  else:
    dict_['Diabetic']+=1
plt.bar(*zip(*dict_.items()))
plt.title("COUNT OF SAMPLES IN TRAINING DATA PER CLASS(AFTER OVERSAMPLING)")
plt.ylabel("Count of Samples")
plt.show()

"""Standaradising the data"""

scale = StandardScaler()
x_train_std = scale.fit_transform(x_train_os)
x_test_std = scale.transform(x_test)

scale1 = StandardScaler()
x_train_std1 = scale1.fit_transform(x_train)
x_test_std1= scale1.transform(x_test)

"""**K-Nearest Neighbours**"""

start_time = time.time()
n_neighbours_list=[]
scoring  = ["accuracy","precision","recall","f1_macro"]
for i in range(1,400):
  n_neighbours_list.append(i)
parameters = {'n_neighbors':n_neighbours_list}
knn = KNeighborsClassifier()
clf_knn = GridSearchCV(knn,parameters,cv=5,refit="accuracy",scoring=scoring)
clf_knn.fit(x_train_std1, y_train)
end_time = time.time()-start_time
print("Time taken is",end_time/60)

clf_knn.best_estimator_

cv_prec_knn = clf_knn.cv_results_['mean_test_precision']
cv_recall_knn = clf_knn.cv_results_['mean_test_recall']
cv_fscore_knn = clf_knn.cv_results_['mean_test_f1_macro']
cv_accs_knn = clf_knn.cv_results_['mean_test_accuracy']

prec_index = np.where(cv_prec_knn==max(cv_prec_knn))
try:
  print("Obtained maximum  precision when number of estimators=",prec_index[0].item()+1)
except:
  print("Obtained maximum  precision when number of estimators=",prec_index.item()+1)
recall_index = np.where(cv_recall_knn==max(cv_recall_knn))
try:
  print("Obtained maximum  recall when number of estimators=",recall_index[0].item()+1)
except:
  print("Obtained maximum  recall when number of estimators=",recall_index.item()+1)  
f1score_index = np.where(cv_fscore_knn==max(cv_fscore_knn))
try:
  print("Obtained maximum  f1-score when number of estimators=",f1score_index[0].item()+1)
except:
  print("Obtained maximum  f1-score when number of estimators=",f1score_index[0]+1)
acc_index = np.where(cv_accs_knn==max(cv_accs_knn))
try:
  print("Obtained maximum  accuracy when number of estimators=",acc_index[0].item()+1)
except:
  print("Obtained maximum  accuracy when number of estimators=",acc_index[0]+1)

print(max(cv_prec_knn))
print(max(cv_recall_knn))
print(max(cv_fscore_knn))
print(max(cv_accs_knn))

"""Graphs with KNN"""

plt.plot(cv_prec_knn,label="Precision on CV data")
plt.plot(cv_recall_knn,label= "Recall on CV data")
plt.plot(cv_fscore_knn,label = "F-Score on CV data")
plt.plot(cv_accs_knn,label = "Accuracy on CV data")
plt.xlabel("No. of NEIGHBOURS")
plt.ylabel("PERFORMANCE ON CV DATA")
plt.title("NUMBER OF NEIGHBOURS  VS PERFORMANCE ON CV DATA:KNN")
plt.legend()
plt.grid(linestyle='-')
plt.show()

"""No. of neighbours vs precision"""

# plt.plot(cv_prec_knn)
# plt.xlabel("No. of NEIGHBOURS")
# plt.ylabel("CV PRECISION")
# plt.title("NUMBER OF NEIGHBOURS  VS PRECISION ON CV DATA :KNN")
# plt.grid(linestyle='-')
# plt.show()

"""No. of neighbours vs recall"""

# plt.plot(cv_recall_knn)
# plt.xlabel("No. of NEIGHBOURS")
# plt.ylabel("CV RECALL")
# plt.title("NUMBER OF NEIGHBOURS  VS RECALL ON CV DATA:KNN")
# plt.grid(linestyle='-')
# plt.show()

"""No. of neighbours vs f-score"""

# plt.plot(cv_fscore_knn)
# plt.xlabel("No. of NEIGHBOURS")
# plt.ylabel("CV F-SCORE")
# plt.title("NUMBER OF NEIGHBOURS  VS F-SCORE ON CV DATA:KNN")
# plt.grid(linestyle='-')
# plt.show()

"""No. of neighbours vs accuracy"""

# plt.plot(cv_accs_knn)
# plt.xlabel("No. of NEIGHBOURS")
# plt.ylabel("CV ACCURACY")
# plt.title("NUMBER OF NEIGHBOURS  VS ACCURACY ON CV DATA:KNN")
# plt.grid(linestyle='-')
# plt.show()

classifier_knn = clf_knn.best_estimator_
classifier_knn.fit(x_train_std1, y_train)
Y_pred = classifier_knn.predict(x_test_std1)

print("Accuracy on test data using KNN::",classifier_knn.score(x_test_std1, y_test)*100)
print("Precision on test data using KNN::",precision_score(y_test, Y_pred, average="macro"))
print("Recall on test data using KNN::",recall_score(y_test, Y_pred, average="macro"))
print("F-Score on test data using KNN::",f1_score(y_test, Y_pred, average="macro"))
spec = confusion_matrix(y_test, Y_pred)[1][1]/(confusion_matrix(y_test, Y_pred)[0][1]+confusion_matrix(y_test, Y_pred)[1][1])
print("Specificity on test data using KNN::",spec)

# pickle_out = open("/content/gdrive/My Drive/BDMH/Project/knn_75.pickle","wb")
# pickle.dump(classifier_knn, pickle_out)

"""**LOGISTIC REGRESSION**"""

lr = Pipeline([('clf_lr', LogisticRegression())])
#param_C = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
param_C = [1,2,3,4,5,6,7,8,9]
param_scoring = ["accuracy","precision","recall","f1_macro"]

param = [{'clf_lr__C': param_C}]
grid_lr = GridSearchCV(estimator=lr,param_grid=param,refit = 'accuracy',scoring=param_scoring,cv=10,n_jobs=-1)
grid_lr = grid_lr.fit(x_train_std1,y_train)
#lr.get_params().keys()

grid_lr.best_estimator_

cv_prec_lr = grid_lr.cv_results_['mean_test_precision']
cv_recall_lr = grid_lr.cv_results_['mean_test_recall']
cv_fscore_lr = grid_lr.cv_results_['mean_test_f1_macro']
cv_accs_lr = grid_lr.cv_results_['mean_test_accuracy']

"""**LR GRAPHS**"""

plt.plot(cv_prec_lr,label="Precision on CV data")
plt.plot(cv_recall_lr,label= "Recall on CV data")
plt.plot(cv_fscore_lr,label = "F-Score on CV data")
plt.plot(cv_accs_lr,label = "Accuracy on CV data")
plt.xlabel("C")
plt.ylabel("PERFORMANCE ON CV DATA")
plt.title("C  VS PERFORMANCE ON CV DATA:LR")
plt.legend()
plt.grid(linestyle='-')
plt.show()

"""C vs recall"""

# plt.plot(cv_recall_lr)

# cval = [1,2,3,4,5,6,7,8,9]
# y_pos = np.arange(len(cval))
# plt.xticks(y_pos,cval)

# plt.xlabel("C Value")
# plt.ylabel("Recall ON CV DATA")
# plt.title("C  V/S CV Recall : LR")
# plt.grid(linestyle="-")
# plt.show()

"""C vs accuracy"""

# plt.plot(cv_accs_lr)

# cval = [1,2,3,4,5,6,7,8,9]
# y_pos = np.arange(len(cval))
# plt.xticks(y_pos,cval)

# plt.xlabel("C Value")
# plt.ylabel("Accuracy ON CV DATA")
# plt.title("C  V/S CV Accuracy : LR")
# plt.grid(linestyle="-")
# plt.show()

"""C vs precision"""

# plt.plot(cv_prec_lr)

# cval = [1,2,3,4,5,6,7,8,9]
# y_pos = np.arange(len(cval))
# plt.xticks(y_pos,cval)

# plt.xlabel("C Value")
# plt.ylabel("Precision ON CV DATA")
# plt.title("C  V/S CV Precision : LR")
# plt.grid(linestyle="-")
# plt.show()

"""C vs f-score"""

# plt.plot(cv_fscore_lr)

# cval = [1,2,3,4,5,6,7,8,9]
# y_pos = np.arange(len(cval))
# plt.xticks(y_pos,cval)

# plt.xlabel("C Value")
# plt.ylabel("FScore ON CV DATA")
# plt.title("C  V/S CV Fscore : LR")
# plt.grid(linestyle="-")
# plt.show()

print(" The Accuracy of the best model is : ",grid_lr.best_score_)
print(" Best Parameters : ",grid_lr.best_params_)

classifier_lr = grid_lr.best_estimator_
classifier_lr.fit(x_train_std1, y_train)
Y_pred = classifier_lr.predict(x_test_std1)

print("Accuracy on test data using Logistic regression::",classifier_lr.score(x_test_std1, y_test)*100)
print("Precision on test data using Logistic Regression::",precision_score(y_test, Y_pred, average="macro"))
print("Recall on test data using Logistic regression::",recall_score(y_test, Y_pred, average="macro"))
print("F-Score on test data using Logistic Regression::",f1_score(y_test, Y_pred, average="macro"))
spec = confusion_matrix(y_test, Y_pred)[1][1]/(confusion_matrix(y_test, Y_pred)[0][1]+confusion_matrix(y_test, Y_pred)[1][1])
print("Specificity on test data using Logistic Regression::",spec)

# pickle_out = open("/content/gdrive/My Drive/BDMH/Project/lr_77.pickle","wb")
# pickle.dump(classifier_lr, pickle_out)





