# -*- coding: utf-8 -*-
"""DIABETES PREDICTION USING ENSEMBLE TECHNIQUES.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d3bEgBKFnDvWU0pJnzR2UKXKG3fbcN-2

## **IMPORTING REQUIRED MODULES**
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import warnings
warnings.filterwarnings("ignore")
from imblearn.over_sampling import ADASYN
from tqdm import tqdm
import pickle 
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
from statistics import mode
from sklearn.metrics import accuracy_score

"""## **READING THE DATA**"""

data = pd.read_csv("/content/drive/My Drive/BDMH_PROJECT/diabetes.csv")
data.head()

"""## **SEPERATING OUT SAMPLES AND LABELS**"""

data_y = pd.DataFrame(data = data['Outcome'],columns=['Outcome'])
data_x = data.drop(columns="Outcome")

data_x.columns

data_y.columns

"""## **PERFORMING EDA (EXPLORATORY DATA ANALYSIS)**

#### **COUNT OF SAMPLES PER CLASS IN THE DATA SET**
"""

data_y['Outcome'].value_counts().plot(kind='bar')
plt.title("INDIVIDUAL COUNT OF LABELS IN THE DATA")
plt.grid()
plt.xticks([0, 1],["Non-Diabetic","Diabetic"],rotation=360)
plt.ylabel("Count of Samples")
plt.show()

"""#### **HISTOGRAM OF THE GIVEN DATA**"""

data_x.hist(figsize=(16,9), edgecolor="black", bins=20)
plt.show()

"""#### **INFERENCES**
1. BLOOD PRESSURE IS ZERO FOR SOME SAMPLES WHICH IS IMPOSSIBLE.
2. BMI IS ZERO FOR SOME SAMPLES WHICH IS NOT POSSIBLE.
3. GLUCOSE IS ZERO FOR SOME SAMPLES WHICH IS TO BE HANDLED.
4. INSULIN IS ZERO FOR SOME SAMPLES WHICH IS NOT POSSIBLE PRACTICALLY.
5. SKIN THICKNESS CANNOT BE ZERO

**THESE ISSUES WILL BE HANDLED USING IMPUTATION METHOD**

## **PERFORMING IMPUTATION ON SPURIOUS SAMPLES FOR FEATURES** 
## **'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI'**

###### In Imputation we replace the invalid values in the dataset by median of the values to which that wrong sample belongs to (Since we have very less data we are going for imputation rather than for elimination of tuples)
"""

#ref:https://towardsdatascience.com/pima-indian-diabetes-prediction-7573698bd5fe
def imputation(data,feature):
  median = data[feature].median()
  print("median of "+str(feature)+"::"+str(median))
  data = data.replace({feature:0},median)
  return data

data = imputation(data,"BloodPressure")
data = imputation(data,"Glucose")
data = imputation(data,"SkinThickness")
data = imputation(data,"Insulin")
## we can still see zeroes in insulin even after imputation for diabetic samples..so we can roughly conclude that diabetics have very low insulin levels..
data = imputation(data,"BMI")

"""### **UNIVARIARE ANALYSIS FOR EVERY FEATURES**"""

def Univar_analysis(data,feature):
  data_nd = data.loc[data["Outcome"]==0] 
  data_d = data.loc[data["Outcome"]==1] 
  sb.set_style("whitegrid") 
  plt.xlabel(feature)
  plt.plot(data_nd[feature],np.zeros_like(data_nd[feature]),marker="x",label='Non-Diabetic',color='red') ## zeros_like will simply create zeros with length same as x cordinate.. 
  plt.plot(data_d[feature],np.zeros_like(data_d[feature]),marker="+",label='Diabetic',color='green') 
  plt.legend()
  plt.title("1-D Scatter plot on feature "+feature)
  plt.show()

Univar_analysis(data,"Age")

Univar_analysis(data,"Pregnancies")

Univar_analysis(data,"Glucose")

Univar_analysis(data,"BloodPressure")

Univar_analysis(data,"SkinThickness")

Univar_analysis(data,"Insulin")

Univar_analysis(data,"BMI")

Univar_analysis(data,"DiabetesPedigreeFunction")

"""## **INFERENCE**
#### No single feature can seperate out or atleast nearly classify one class from the another, because there is considerable amount of overlap between the points of both the classes in all the features.

## **THE PAIR-PLOT**
"""

sb.pairplot(data,hue="Outcome")
plt.show()

"""## **INFERENCE**
The combination of any two features cannot even decently classify the data. As there is high amount of overlap between the PDFs of every two features.

## **SPLITTING THE DATA INTO TRAIN AND TEST**
"""

x_train,x_test,y_train,y_test = train_test_split(data_x,data_y,train_size=0.70,random_state=0)#45
print("SHAPE OF X-TRAIN::",x_train.shape)
print("SHAPE OF Y-TRAIN::",y_train.shape)
print("SHAPE OF X-TEST::",x_test.shape)
print("SHAPE OF Y-TEST::",y_test.shape)
y_train['Outcome'].value_counts().plot(kind='bar')
plt.title("COUNT OF SAMPLES IN TRAINING DATA PER CLASS(BEFORE OVERSAMPLING)")
plt.xticks([0, 1],["Non-Diabetic","Diabetic"],rotation=360)
plt.ylabel("Count of Samples")
plt.show()

#x_train.to_csv('/content/drive/My Drive/BDMH_PROJECT/xtrain.csv') 
#x_test.to_csv('/content/drive/My Drive/BDMH_PROJECT/xtest.csv') 
#y_train.to_csv('/content/drive/My Drive/BDMH_PROJECT/ytrain.csv') 
#y_test.to_csv('/content/drive/My Drive/BDMH_PROJECT/ytest.csv')

"""## **OVERSAMPLING THE DATA USING ADASYN**"""

## oversampling is performed only on training data...
oversample = ADASYN(random_state=10,sampling_strategy="minority")
x_train_os, y_train_os = oversample.fit_sample(x_train,y_train)
dict_={"Non-Diabetic":0,"Diabetic":0}
for i in y_train_os:
  if(i==0):
    dict_['Non-Diabetic']+=1
  else:
    dict_['Diabetic']+=1
plt.bar(*zip(*dict_.items()))
plt.title("COUNT OF SAMPLES IN TRAINING DATA PER CLASS(AFTER OVERSAMPLING)")
plt.ylabel("Count of Samples")
plt.show()

"""## **STANDARDISING THE DATA**"""

scale = StandardScaler()
x_train_std = scale.fit_transform(x_train_os)
x_test_std = scale.transform(x_test)

scale.mean_,scale.var_

print(scale.mean_)

x_test_std[1]

def scale_data(array):
    means = np.array([  4.061425  , 122.98923755,  70.26122953,  21.45881463,88.24362615,  32.70515732,   0.48941921,  34.41192381])
    stds=   np.array([11.4233741,975.386508,349.351072, 250.185968,14964.9958,57.4279121,.110725199, 136.299159]) **0.5
    return (array-means)/stds

scale_data(np.array([2.0,107.0,74.0,30.0,100.6,33.6,0.40399999999999997,23.0]).reshape(1,8))

#pickle_out = open("/content/drive/My Drive/BDMH_PROJECT/std.pickle","wb")
#pickle.dump(scale, pickle_out)

"""## **APPLYING RANDOM FOREST CLASSIFIER**

#### **USING GRIDSEARCHCV FOR HYPER-PARAMTER TUNING**
"""

scoring  = ["accuracy","precision","recall","f1_macro"]
n_estm_list=[]
for i in range(1,200):
  n_estm_list.append(i)
parameters = {'n_estimators':n_estm_list}
rand_forest = RandomForestClassifier(random_state=10)
clf = GridSearchCV(rand_forest,parameters,cv=5,refit="accuracy",scoring=scoring)
clf.fit(x_train_std, y_train_os)

clf.best_estimator_

"""#### **STORING THE LISTS TO GENERATE PLOTS**"""

cv_prec = clf.cv_results_['mean_test_precision']
cv_recall = clf.cv_results_['mean_test_recall']
cv_fscore = clf.cv_results_['mean_test_f1_macro']
cv_accs = clf.cv_results_['mean_test_accuracy']

"""#### **RESULTS ON CV DATA**"""

prec_index = np.where(cv_prec==max(cv_prec))
try:
  print("Obtained maximum  precision when number of estimators=",prec_index[0].item()+1)
except:
  print("Obtained maximum  precision when number of estimators=",prec_index.item()+1)
recall_index = np.where(cv_recall==max(cv_recall))
try:
  print("Obtained maximum  recall when number of estimators=",recall_index[0].item()+1)
except:
  print("Obtained maximum  recall when number of estimators=",recall_index.item()+1)  
f1score_index = np.where(cv_fscore==max(cv_fscore))
try:
  print("Obtained maximum  f1-score when number of estimators=",f1score_index[0].item()+1)
except:
  print("Obtained maximum  f1-score when number of estimators=",f1score_index[0]+1)
acc_index = np.where(cv_accs==max(cv_accs))
try:
  print("Obtained maximum  accuracy when number of estimators=",acc_index[0].item()+1)
except:
  print("Obtained maximum  accuracy when number of estimators=",acc_index[0]+1)

"""## **NUMBER OF ESTIMATORS(TREES) VS PERFORMANCE METRICS**"""

plt.plot(cv_prec,label="Precision on CV data")
plt.plot(cv_recall,label= "Recall on CV data")
plt.plot(cv_fscore,label = "F-Score on CV data")
plt.plot(cv_accs,label = "Accuracy on CV data")
plt.xlabel("No. of ESTIMATORS")
plt.ylabel("PERFORMANCE ON CV DATA")
plt.title("NUMBER OF ESTIMATORS  VS PERFORMANCE ON CV DATA")
plt.legend()
plt.grid(linestyle='-')
plt.show()

"""## **INFERENCE**
In most of the metric plots we see the peak when number of estimators are between 90 to 140 and also when number of estimators are between 175 to 200. so we tried out various count of estimators from that ranges.

## **THE OPTIMAL MODEL (RANDOM FOREST)**
"""

##  extracting the hints from  gridsearch i tried on other values of hyper-paramters as well
rand_forest = RandomForestClassifier(n_estimators=181,random_state=10)
rand_forest.fit(x_train_std, y_train_os)
y_pred = rand_forest.predict(x_test_std)
print("Accuracy on test data using random-forest classifier::",rand_forest.score(x_test_std, y_test)*100)
print("Precision on test data using random-forest classifier::",precision_score(y_test, y_pred, average="macro"))
print("Recall on test data using random-forest classifier::",recall_score(y_test, y_pred, average="macro"))
print("F-Score on test data using random-forest classifier::",f1_score(y_test, y_pred, average="macro"))
spec = confusion_matrix(y_test, y_pred)[1][1]/(confusion_matrix(y_test, y_pred)[0][1]+confusion_matrix(y_test, y_pred)[1][1])
print("Specificity on test data using random-forest classifier::",spec)

"""## **SAVING THE OPTIMAL MODEL(RANDOM FOREST CLASSIFIER)**"""

#pickle_out = open("/content/drive/My Drive/BDMH_PROJECT/random_forest_80_test.pickle","wb")
#pickle.dump(rand_forest, pickle_out)

"""## **APPLYING ADABOOST CLASSIFIER**

#### **USING GRIDSEARCHCV FOR HYPER-PARAMTER TUNING**
"""

n_estm_list_adaboost=[]
scoring  = ["accuracy","precision","recall","f1_macro"]
for i in range(1,400):
  n_estm_list_adaboost.append(i)
parameters_adaboost = {'n_estimators':n_estm_list_adaboost}
ada_boost = AdaBoostClassifier(random_state=10)
clf = GridSearchCV(ada_boost,parameters_adaboost,cv=5,refit="accuracy",scoring=scoring)
clf.fit(x_train_std, y_train_os)

clf.best_estimator_

"""#### **STORING THE LISTS TO GENERATE PLOTS**"""

cv_prec_adaboost = clf.cv_results_['mean_test_precision']
cv_recall_adaboost = clf.cv_results_['mean_test_recall']
cv_fscore_adaboost = clf.cv_results_['mean_test_f1_macro']
cv_accs_adaboost = clf.cv_results_['mean_test_accuracy']

"""#### **RESULTS ON CV DATA**"""

prec_index = np.where(cv_prec_adaboost==max(cv_prec_adaboost))
try:
  print("Obtained maximum  precision when number of estimators=",prec_index[0].item()+1)
except:
  print("Obtained maximum  precision when number of estimators=",prec_index[0]+1)
recall_index = np.where(cv_recall_adaboost==max(cv_recall_adaboost))
try:
  print("Obtained maximum  recall when number of estimators=",recall_index[0].item()+1)
except:
  print("Obtained maximum  recall when number of estimators=",recall_index.item()+1)
f1score_index = np.where(cv_fscore_adaboost==max(cv_fscore_adaboost))
try:
  print("Obtained maximum  f1-score when number of estimators=",f1score_index[0].item()+1)
except:
  print("Obtained maximum  f1-score when number of estimators=",f1score_index[0]+1)
acc_index = np.where(cv_accs_adaboost==max(cv_accs_adaboost))
try:
  print("Obtained maximum  accuracy when number of estimators=",acc_index[0].item()+1)
except:
  print("Obtained maximum  accuracy when number of estimators=",acc_index[0]+1)

"""## **NUMBER OF ESTIMATORS V/S PRECISION**"""

plt.plot(cv_prec_adaboost,label="Precision on CV data")
plt.plot(cv_recall_adaboost,label="Recall on CV data")
plt.plot(cv_fscore_adaboost,label="F-Score on CV data")
plt.plot(cv_accs_adaboost,label="Accuracy on CV data")
plt.xlabel("No. of ESTIMATORS")
plt.ylabel("PERFORMANCE ON CV DATA")
plt.title("NUMBER OF ESTIMATORS  VS PERFORMANCE ON CV DATA")
plt.legend()
plt.grid(linestyle='-')
plt.show()

"""## **INFERENCE**
In most of the metric plots we see the peak when number of estimators are between 50 to 75 and also when number of estimators are between 175 to 225. so we tried out various count of estimators from that ranges.

## **THE OPTIMAL MODEL(ADABOOST)**
"""

ada_boost = AdaBoostClassifier(n_estimators=74, random_state=10)
ada_boost.fit(x_train_std, y_train_os)
y_pred = ada_boost.predict(x_test_std)
print("Accuracy on test data using adaboost classifier::",ada_boost.score(x_test_std, y_test)*100)
print("Precision on test data using adaboost classifier::",precision_score(y_test, y_pred, average="macro"))
print("Recall on test data using adaboost classifier::",recall_score(y_test, y_pred, average="macro"))
print("F-Score on test data using adaboost classifier::",f1_score(y_test, y_pred, average="macro"))
spec = confusion_matrix(y_test, y_pred)[1][1]/(confusion_matrix(y_test, y_pred)[0][1]+confusion_matrix(y_test, y_pred)[1][1])
print("Specificity on test data using adaboost classifier::",spec)

"""## **SAVING THE OPTIMAL MODEL(ADABOOST CLASSIFIER)**"""

#pickle_out = open("/content/drive/My Drive/BDMH_PROJECT/adaboost_79_point_65_test.pickle","wb")
#pickle.dump(ada_boost, pickle_out)

"""## **APPLYING XGBOOST CLASSIFIER**

#### **USING GRIDSEARCHCV FOR HYPER-PARAMETER TUNING**
"""

n_estm_list_xgboost=[]
scoring  = ["accuracy","precision","recall","f1_macro"]
for i in range(1,401):
  n_estm_list_xgboost.append(i)
parameters_xgboost = {'n_estimators':n_estm_list_xgboost}
xg_boost = XGBClassifier(random_state=10)
clf = GridSearchCV(xg_boost,parameters_xgboost,cv=5,refit="accuracy",scoring=scoring)
clf.fit(x_train_std, y_train_os)

clf.best_estimator_

"""#### **STORING THE LISTS TO GENERATE PLOTS**"""

cv_prec_xgboost = clf.cv_results_['mean_test_precision']
cv_recall_xgboost = clf.cv_results_['mean_test_recall']
cv_fscore_xgboost = clf.cv_results_['mean_test_f1_macro']
cv_accs_xgboost = clf.cv_results_['mean_test_accuracy']

"""#### **RESULTS ON CV DATA**"""

prec_index = np.where(cv_prec_xgboost==max(cv_prec_xgboost))
try:
  print("Obtained maximum  precision when number of estimators=",prec_index[0].item()+1)
except:
  print("Obtained maximum  precision when number of estimators=",prec_index[0]+1)
recall_index = np.where(cv_recall_xgboost==max(cv_recall_xgboost))
try:
  print("Obtained maximum  recall when number of estimators=",recall_index[0].item()+1)
except:
  print("Obtained maximum  recall when number of estimators=",recall_index.item()+1)
f1score_index = np.where(cv_fscore_xgboost==max(cv_fscore_xgboost))
try:
  print("Obtained maximum  f1-score when number of estimators=",f1score_index[0].item()+1)
except:
  print("Obtained maximum  f1-score when number of estimators=",f1score_index[0]+1)
acc_index = np.where(cv_accs_xgboost==max(cv_accs_xgboost))
try:
  print("Obtained maximum  accuracy when number of estimators=",acc_index[0].item()+1)
except:
  print("Obtained maximum  accuracy when number of estimators=",acc_index[0]+1)

"""## **NUMBER OF ESTIMATORS V/S PERFORMANCE**"""

plt.plot(cv_prec_xgboost,label="Precision on CV data")
plt.plot(cv_recall_xgboost,label ="Recall on CV data")
plt.plot(cv_fscore_xgboost,label="F-Score on CV data")
plt.plot(cv_accs_xgboost,label="Accuracy on CV data")
plt.xlabel("No. of ESTIMATORS")
plt.ylabel("PERFORMANCE ON CV DATA")
plt.legend()
plt.title("NUMBER OF ESTIMATORS  V/S PERFORMANCE ON CV DATA")
plt.grid(linestyle="-")
plt.show()

"""## **INFERENCE**
In most of the metric plots the performance is better when number of estimators are beyond 300. So we tried various values from 300 to beyond 300.

## **THE OPTIMAL MODEL(XGBOOST CLASSIFIER)**
"""

xg_boost = XGBClassifier(n_estimators=62, random_state=10)
xg_boost.fit(x_train_std, y_train_os)
y_pred = xg_boost.predict(x_test_std)
print("Accuracy on test data using XGBoost Classifier::",xg_boost.score(x_test_std, y_test)*100)
print("Precision on test data using XGBoost Classifier::",precision_score(y_test, y_pred, average="macro"))
print("Recall on test data using XGBoost Classifier::",recall_score(y_test, y_pred, average="macro"))
print("F-Score on test data using XGBoost Classifier::",f1_score(y_test, y_pred, average="macro"))
spec = confusion_matrix(y_test, y_pred)[1][1]/(confusion_matrix(y_test, y_pred)[0][1]+confusion_matrix(y_test, y_pred)[1][1])
print("Specificity on test data using XGBoost Classifier::",spec)

"""## **SAVING THE OPTIMAL MODEL(XGBOOST CLASSIFIER)**"""

#pickle_out = open("/content/drive/My Drive/BDMH_PROJECT/xgboost_78_point_78_test.pickle","wb")
#pickle.dump(xg_boost, pickle_out)
#xg_boost.save_model('xgboost_78.json')

# save
#pickle.dump(xg_boost, open(, "wb"))

import joblib
#save model
joblib.dump(xg_boost, "/content/drive/My Drive/BDMH_PROJECT/xgboost") 

# load
#xgb_model_loaded = pickle.load(open(file_name, "rb"))

"""## **APPLYING GRADIENT BOOSTING CLASSIFIER**

#### **USING GRIDSEARCHCV FOR HYPER-PARAMETER TUNING**
"""

n_estm_list_gradboost=[]
scoring  = ["accuracy","precision","recall","f1_macro"]
for i in range(1,401):
  n_estm_list_gradboost.append(i)
parameters_gradboost = {'n_estimators':n_estm_list_gradboost}
grad_boost = GradientBoostingClassifier(learning_rate=0.09,random_state=10)
clf = GridSearchCV(grad_boost,parameters_gradboost,cv=5,refit="accuracy",scoring=scoring)
clf.fit(x_train_std, y_train_os)

clf.best_estimator_

"""#### **STORING THE LISTS TO GENERATE PLOTS**"""

cv_prec_gradboost = clf.cv_results_['mean_test_precision']
cv_recall_gradboost = clf.cv_results_['mean_test_recall']
cv_fscore_gradboost = clf.cv_results_['mean_test_f1_macro']
cv_accs_gradboost = clf.cv_results_['mean_test_accuracy']

"""#### **RESULTS ON CV DATA**"""

prec_index = np.where(cv_prec_gradboost==max(cv_prec_gradboost))
try:
  print("Obtained maximum  precision when number of estimators=",prec_index[0].item()+1)
except:
  print("Obtained maximum  precision when number of estimators=",prec_index[0]+1)
recall_index = np.where(cv_recall_gradboost==max(cv_recall_gradboost))
try:
  print("Obtained maximum  recall when number of estimators=",recall_index[0].item()+1)
except:
  print("Obtained maximum  recall when number of estimators=",recall_index.item()+1)
f1score_index = np.where(cv_fscore_gradboost==max(cv_fscore_gradboost))
try:
  print("Obtained maximum  f1-score when number of estimators=",f1score_index[0].item()+1)
except:
  print("Obtained maximum  f1-score when number of estimators=",f1score_index[0]+1)
acc_index = np.where(cv_accs_gradboost==max(cv_accs_gradboost))
try:
  print("Obtained maximum  accuracy when number of estimators=",acc_index[0].item()+1)
except:
  print("Obtained maximum  accuracy when number of estimators=",acc_index[0]+1)

"""## **NUMBER OF ESTIMATORS V/S PERFORMANCES**"""

plt.plot(cv_prec_gradboost,label="Precision on CV data")
plt.plot(cv_recall_gradboost,label="Recall on CV data")
plt.plot(cv_fscore_gradboost,label="F-Score on CV data")
plt.plot(cv_accs_gradboost,label="Accuracy on CV data")
plt.xlabel("No. of ESTIMATORS")
plt.ylabel("PERFORMANCE ON CV DATA")
plt.title("NUMBER OF ESTIMATORS  V/S PERFORMANCE ON CV DATA")
plt.legend()
plt.grid(linestyle="-")
plt.show()

"""## **INFERENCE**
In most of the plots there is a performance rise when number of estimators are between 125 to 175 so we tried out various values from that range to obtain optimal performance.

## **OPTIMAL MODEL (GRADIENT BOOSTING CLASSIFIER)**
"""

grad_boost = GradientBoostingClassifier(n_estimators=140,learning_rate=0.09,random_state=10)
grad_boost.fit(x_train_std, y_train_os)
y_pred = grad_boost.predict(x_test_std)
print("Accuracy on test data using Gradient-Boosting Classifier::",grad_boost.score(x_test_std, y_test)*100)
print("Precision on test data using Gradient-Boosting Classifier::",precision_score(y_test, y_pred, average="macro"))
print("Recall on test data using Gradient-Boosting Classifier::",recall_score(y_test, y_pred, average="macro"))
print("F-Score on test data using Gradient-Boosting Classifier::",f1_score(y_test, y_pred, average="macro"))
spec = confusion_matrix(y_test, y_pred)[1][1]/(confusion_matrix(y_test, y_pred)[0][1]+confusion_matrix(y_test, y_pred)[1][1])
print("Specificity on test data using Gradient-Boosting Classifier::",spec)

"""## **SAVING THE OPTIMAL MODEL (GRADIENT BOOSTING CLASSIFIER)**"""

#pickle_out = open("/content/drive/My Drive/BDMH_PROJECT/grad_boost_79_point_2_test.pickle","wb")
#pickle.dump(grad_boost, pickle_out)

"""## **VOTING CLASSIFIER**"""

pickle_in = open("/content/drive/My Drive/BDMH_PROJECT/adaboost_79_point_65_test.pickle","rb")
adaboost_ = pickle.load(pickle_in)

pickle_in = open("/content/drive/My Drive/BDMH_PROJECT/best_gnb_clf.pkl","rb")
naivebayes_ = pickle.load(pickle_in)

pickle_in = open("/content/drive/My Drive/BDMH_PROJECT/gpc_clf.pkl","rb")
gaussprocess_ = pickle.load(pickle_in)

pickle_in = open("/content/drive/My Drive/BDMH_PROJECT/dt_72_acc.pickle","rb")
dectrees_ = pickle.load(pickle_in)

pickle_in = open("/content/drive/My Drive/BDMH_PROJECT/grad_boost_79_point_2_test.pickle","rb")
gradboost_ = pickle.load(pickle_in)

pickle_in = open("/content/drive/My Drive/BDMH_PROJECT/knn_75.pickle","rb")
knn_ = pickle.load(pickle_in)

pickle_in = open("/content/drive/My Drive/BDMH_PROJECT/logistic_77.pickle","rb")
logistic_ = pickle.load(pickle_in)

pickle_in = open("/content/drive/My Drive/BDMH_PROJECT/random_forest_80_test.pickle","rb")
randforest_ = pickle.load(pickle_in)

pickle_in = open("/content/drive/My Drive/BDMH_PROJECT/svmlinear_78_acc.pickle","rb")
svmlin_ = pickle.load(pickle_in)

pickle_in = open("/content/drive/My Drive/BDMH_PROJECT/svmrbf_75_acc.pickle","rb")
svmrbf_ = pickle.load(pickle_in)

pickle_in = open("/content/drive/My Drive/BDMH_PROJECT/xgboost_78_point_78_test.pickle","rb")
xgboost_ = pickle.load(pickle_in)

l1 = list(adaboost_.predict(x_test_std))
l2 = list(xgboost_.predict(x_test_std)) 
l3 = list(gradboost_.predict(x_test_std)) 
l4 = list(randforest_.predict(x_test_std)) 
l5 = list(knn_.predict(x_test_std))
l6 = list(logistic_.predict(x_test_std))
l7 = list(svmlin_.predict(x_test_std))
l8 = list(svmrbf_.predict(x_test_std))
l9 = list(dectrees_.predict(x_test_std))
l10 = list(naivebayes_.predict(x_test_std))
l11 = list(gaussprocess_.predict(x_test_std))

y_pred_vote = []
for i in range(len(l1)):
  y_pred_vote.append(mode([l1[i],l3[i],l4[i]]))
print("Accuracy on test data using Voting Classifier::",accuracy_score(y_test,y_pred_vote)*100)
print("Precision on test data using Voting Classifier::",precision_score(y_test, y_pred_vote, average="macro"))
print("Recall on test data using Voting Classifier::",recall_score(y_test, y_pred_vote, average="macro"))
print("F-Score on test data using Voting Classifier::",f1_score(y_test, y_pred_vote, average="macro"))
spec = confusion_matrix(y_test, y_pred_vote)[1][1]/(confusion_matrix(y_test,y_pred_vote)[0][1]+confusion_matrix(y_test, y_pred_vote)[1][1])
print("Specificity on test data using Voting Classifier::",spec)

adaboost_.predict_proba([[1,2,3,4,5,6,7,8]]),adaboost_.predict_proba([[1,2,3,4,5,6,7,8]])[0][0],adaboost_.predict_proba([[1,2,3,4,5,6,7,8]])[0][1]

adaboost_.predict([[1,2,3,4,5,6,7,8]])[0]

